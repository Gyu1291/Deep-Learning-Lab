{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3B60UvFA5EB"
   },
   "source": [
    "> ### EEE4423: Deep Learning Lab\n",
    "\n",
    "# LAB \\#3: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-02-25T09:39:30.171355Z",
     "iopub.status.busy": "2022-02-25T09:39:30.171242Z",
     "iopub.status.idle": "2022-02-25T09:39:30.176763Z",
     "shell.execute_reply": "2022-02-25T09:39:30.175705Z",
     "shell.execute_reply.started": "2022-02-25T09:39:30.171340Z"
    },
    "id": "0MuQ_XTtA5ED",
    "outputId": "6936d214-053f-422f-8a04-cd107cbfade8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is written at 2022-02-25 09:39:30.172001\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6uzxe1dA5EJ"
   },
   "source": [
    "# Convolutional Neural Network with PyTorch\n",
    "\n",
    "## 1. About Convolutional Neural Network\n",
    "\n",
    "\n",
    "### 1.1 Transition From Feedforward Neural Network\n",
    "\n",
    "#### Hidden Layer Feedforward Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtR4seKHA5EK"
   },
   "source": [
    "> **Recap of FNN**\n",
    ">\n",
    "> So let's do a recap of what we covered in the Feedforward Neural Network (FNN) section using a simple FNN with 1 hidden layer (a pair of affine function and non-linear function)\n",
    ">    \n",
    "> 1. [Yellow box] Pass input into an affine function $\\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b}$\n",
    "> 2. [Pink box] Pass logits to non-linear function, for example sigmoid, tanh (hyperbolic tangent), ReLU, or LeakyReLU\n",
    "> 3. [Blue box] Pass output of non-linear function to another affine function\n",
    "> 4. [Red box] Pass output of final affine function to softmax function to get our probability distribution over K classes\n",
    "> 5. [Purple box] Finally we can get our loss by using our cross entropy function \n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1bvMjblA1vOzghVsCALdzknmQyYgotWQh\" alt=\"no_image\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0cOmJC6A5EL"
   },
   "source": [
    "#### Basic Convolutional Neural Network (CNN)\n",
    "\n",
    "- A basic CNN just requires 2 additional layers!\n",
    "    - **Convolution** and **pooling** layers **before our feedforward neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jB1FulBdA5EN"
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=17Kqhr497dI8fvm_Q4gs8MY5k8qJiu95z\" alt=\"no_image\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DrXjsNI1A5EO"
   },
   "source": [
    "### 1.2 One Convolutional Layer: High Level View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iF4xVex2A5EP"
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=14z2BzTdgGbX7KY826lpgBIJegTcD0qxa\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=11OWj5PFD7i22GaSoNruTu70SbrxesPQx\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=14z2BzTdgGbX7KY826lpgBIJegTcD0qxa\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1t0LEBHcVh8OzJP_q303_Vmj3YLB8Xebw\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1rFPdCPwMArBpAhWrLw2VjCo-OA6z5esZ\" alt=\"no_image\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zaHBf8l_A5EQ",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.3 One Convolutional Layer: High Level View Summary\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=14z2BzTdgGbX7KY826lpgBIJegTcD0qxa\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "- As the **kernel is sliding/convolving** across the image $\\rightarrow$ 2 operations done **per patch**\n",
    "    1. Element-wise multiplication\n",
    "    2. Summation\n",
    "- More **kernels** $=$ more **feature map channels**\n",
    "    - Can capture **more information** about the input\n",
    "    \n",
    "\n",
    "### 1.4 Multiple Convolutional Layers: High Level View\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1X7v95vTFu5ZkS7H358-JK_fSiTS9GHou\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "### 1.5 Pooling Layer: High Level View\n",
    "- 2 Common Types\n",
    "    - Max Pooling\n",
    "    - Average Pooling\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=14Ppj52QvlrvL38DdGHCf5e69Img89OOZ\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "### 1.6 Multiple Pooling Layers: High Level View\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1X7v95vTFu5ZkS7H358-JK_fSiTS9GHou\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "### 1.7 Padding\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1ABrZpg_Kgp-SXcwEexAl50a0x8jDMEXJ\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "### 1.8 Padding Summary\n",
    "- **Valid** Padding (Zero Padding)\n",
    "    - Output size < Input Size\n",
    "- **Same** Padding\n",
    "    - Output size = Input Size\n",
    "\n",
    "### 1.9 Dimension Calculations\n",
    "- $ O = \\frac {W - K + 2P}{S} + 1$\n",
    "    - $O$: output height/length\n",
    "    - $W$: input height/length\n",
    "    - $K$: filter size (kernel size)\n",
    "    - $P$: padding\n",
    "        - $ P = \\frac{K - 1}{2} $\n",
    "    - $S$: stride\n",
    "    \n",
    "#### Example 1: Output Dimension Calculation for Valid Padding\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1VcyULOZoF2pELwA5DYeqHVij10vQJgTS\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "- $W = 4$\n",
    "- $K = 3$\n",
    "- $P = 0$\n",
    "- $S = 1$\n",
    "- $O = \\frac {4 - 3 + 2*0}{1} + 1 = \\frac {1}{1} + 1 = 1 + 1 = 2 $\n",
    "\n",
    "\n",
    "#### Example 2: Output Dimension Calculation for Same Padding\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1YbAZNmJK1DBA3swKg8muUis90JDmcQDm\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "- $W = 5$\n",
    "- $K = 3$\n",
    "- $P = \\frac{3 - 1}{2} = \\frac{2}{2} = 1 $\n",
    "- $S = 1 $\n",
    "- $O = \\frac {5 - 3 + 2*1}{1} + 1 = \\frac {4}{1} + 1 = 5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QX-UGaLJA5ER"
   },
   "source": [
    "## 2. Building a Convolutional Neural Network with PyTorch\n",
    "\n",
    "### Model A: \n",
    "- 2 Convolutional Layers\n",
    "    - Same Padding (same output size)\n",
    "- 2 Max Pooling Layers\n",
    "- 1 Fully Connected Layer\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1xQzHZ7okBvBGmRgfLH1dd95onwdTG6Gv\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- Step 3: Create Model Class\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPDhJ6T4A5ES"
   },
   "source": [
    "### Step 1: Loading MNIST Train Dataset\n",
    "#### Images from 1 to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6Vpgj5FA5ES"
   },
   "source": [
    "**MNIST Dataset and Size of Training Dataset (Excluding Labels)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XzpjrB-uA5ET",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z0a1R8eGA5EW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='../dataset/lab03', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='../dataset/lab03', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iGOPMLaBA5EZ",
    "outputId": "84582421-5747-4f4b-bd94-1e2866ff42df",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LRiLSALmA5Ec"
   },
   "source": [
    "**Size of our training dataset labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9Kat_IlA5Ed",
    "outputId": "91c27ce2-1892-498d-f243-ce4b647d7cba",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.targets.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-INVWOEA5Eg"
   },
   "source": [
    "**Size of our testing dataset (excluding labels)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0QOW_C4OA5Ei",
    "outputId": "bd434485-325d-47e8-ae92-fe00865f49ea",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGLRUKuSA5El"
   },
   "source": [
    "**Size of our testing dataset labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySJwCIr-A5En",
    "outputId": "243aad78-81f1-48b2-d6cb-16e278d61595",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.targets.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHsWPONMA5Eq"
   },
   "source": [
    "### Step 2: Make Dataset Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAV8It8UA5Er"
   },
   "source": [
    "**Load Dataset into Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UU0qxR5eA5Es",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ciztnPfDA5Ev",
    "outputId": "7ea1579f-49ae-435b-aad2-9b7150a8219d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2tNNXPPA5Ez"
   },
   "source": [
    "### Step 3: Create Model Class\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1xQzHZ7okBvBGmRgfLH1dd95onwdTG6Gv\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "#### Output Formula for Convolution\n",
    "- $ O = \\frac {W - K + 2P}{S} + 1$\n",
    "    - $O$: output height/length\n",
    "    - $W$: input height/length\n",
    "    - $K$: **filter size (kernel size) = 5**\n",
    "    - $P$: **same padding (non-zero)**\n",
    "        - $P = \\frac{K - 1}{2}  = \\frac{5 - 1}{2} = 2$\n",
    "    - $S$: **stride = 1**\n",
    "    \n",
    "#### Output Formula for Pooling\n",
    "- $ O = \\frac {W - K}{S} + 1$\n",
    "    - W: input height/width\n",
    "    - K: **filter size = 2**\n",
    "    - S: **stride size = filter size**, PyTorch defaults the stride to kernel filter size\n",
    "        - If using PyTorch default stride, this will result in the formula $ O = \\frac {W}{K}$\n",
    "        - By default, in our tutorials, we do this for simplicity.\n",
    "    \n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1EVR_GeSZhE_gRPw0kjDtzd0rVG9ccuOU\" alt=\"no_image\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXY2RNw8A5E0"
   },
   "source": [
    "**Define our simple 2 convolutional layer CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VtzUznyzA5E1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "\n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "\n",
    "        # Resize\n",
    "        # Original size: (100, 32, 7, 7)\n",
    "        # out.size(0): 100\n",
    "        # New out size: (100, 32*7*7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PfP94uK-A5E5"
   },
   "source": [
    "### Step 4: Instantiate Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2ydgaSbA5E7"
   },
   "source": [
    "**Our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uh13QyQnA5E9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0ce_8h7A5FC"
   },
   "source": [
    "### Step 5: Instantiate Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AjVQhzDtA5FC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cXX8ZxDwA5FG"
   },
   "source": [
    "### Step 6: Instantiate Optimizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-Rda-v4A5FG"
   },
   "source": [
    "- Simplified equation\n",
    "    - $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta$\n",
    "        - $\\theta$: parameters\n",
    "        - $\\eta$: learning rate (how fast we want to learn)\n",
    "        - $\\nabla_\\theta$: parameters' gradients\n",
    "- Even simplier equation\n",
    "    - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    - **At every iteration, we update our model's parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNXSsrSmA5FG"
   },
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIs775ZfA5FH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVJx5x9tA5FJ"
   },
   "source": [
    "### Parameters In-Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbx665D3A5FK"
   },
   "source": [
    "**Print model's parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_2U8s_nA5FK",
    "outputId": "16462a95-3550-450e-b800-7c9d76a77871",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7fcda8301c50>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hs3Y1XFZA5FN",
    "outputId": "611cac59-c9e6-4886-ad5e-627c09cbbe22",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lTNj170hA5FR",
    "outputId": "9bb7b475-5dab-46c4-bad1-7242077abb49",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Convolution 1: 16 Kernels\n",
    "print(list(model.parameters())[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T0ByiWfZA5FV",
    "outputId": "a265d613-5851-47a2-a163-17ebbc1d0ce7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Convolution 1 Bias: 16 Kernels\n",
    "print(list(model.parameters())[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKgK_-OxA5FZ",
    "outputId": "fddcb9a5-3097-4bff-d01d-f3529cabe923",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Convolution 2: 32 Kernels with depth = 16\n",
    "print(list(model.parameters())[2].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ex8vaY90A5Fb",
    "outputId": "e9dd5a5b-7a69-42d0-bbe6-614932ef0562",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Convolution 2 Bias: 32 Kernels with depth = 16\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-ahcxWqA5Fe",
    "outputId": "a7b2979e-2974-4c71-835a-e43943c5333e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1568])\n"
     ]
    }
   ],
   "source": [
    "# Fully Connected Layer 1\n",
    "print(list(model.parameters())[4].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bp2E3yErA5Fh",
    "outputId": "00029de5-7cd9-4331-8be7-61c9ccf2286b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Fully Connected Layer Bias\n",
    "print(list(model.parameters())[5].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjZEsuW6A5Fk"
   },
   "source": [
    "### Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pVjKjbiAA5Fk"
   },
   "source": [
    "- Process \n",
    "    1. **Convert inputs to tensors with gradient accumulation abilities**\n",
    "        - CNN Input: (1, 28, 28) \n",
    "        - Feedforward NN Input: (1, 28*28)\n",
    "    2. Clear gradient buffets\n",
    "    3. Get output given inputs \n",
    "    4. Get loss\n",
    "    5. Get gradients w.r.t. parameters\n",
    "    6. Update parameters using gradients\n",
    "        - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    7. REPEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9e1dkyTIA5Fk"
   },
   "source": [
    "**Model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DRZxWQkA5Fm",
    "outputId": "724505fa-1048-45ca-c0b1-d7ddca7851bb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.35360896587371826. Accuracy: 89.33\n",
      "Iteration: 1000. Loss: 0.3857906758785248. Accuracy: 92.49\n",
      "Iteration: 1500. Loss: 0.1491824984550476. Accuracy: 94.79\n",
      "Iteration: 2000. Loss: 0.055039744824171066. Accuracy: 95.59\n",
      "Iteration: 2500. Loss: 0.1522841900587082. Accuracy: 96.3\n",
      "Iteration: 3000. Loss: 0.34792858362197876. Accuracy: 96.91\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images) # 100 x 10\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy\n",
    "            model.eval()\n",
    "            \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_VqlmwMA5Fo"
   },
   "source": [
    "### Model B: \n",
    "- 2 Convolutional Layers\n",
    "    - Same Padding (same output size)\n",
    "- 2 **Average Pooling** Layers\n",
    "- 1 Fully Connected Layer\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1QLYPmAgR9PLIZH4TUBhqA2vvPH3GO4ql\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=15HExUU-nvnvcPYe2EcmwBqSo0rzbrnci\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- Step 3: Create Model Class\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ugHaGhkzA5Fp"
   },
   "source": [
    "**2 Conv + 2 Average Pool + 1 FC (Zero Padding, Same Padding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-02-23T08:39:33.980384Z",
     "iopub.status.busy": "2022-02-23T08:39:33.979888Z",
     "iopub.status.idle": "2022-02-23T08:40:23.293543Z",
     "shell.execute_reply": "2022-02-23T08:40:23.291529Z",
     "shell.execute_reply.started": "2022-02-23T08:39:33.980330Z"
    },
    "id": "Ccmckw-7A5Fq",
    "outputId": "69d42c9a-a199-4c1e-fe71-7c5a939ad652",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: 2.3069164752960205. Accuracy: 15.05\n",
      "Iteration: 2. Loss: 2.2975988388061523. Accuracy: 15.09\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3618/1150852034.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# Iterate through test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# Forward pass only to get logits/output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mgetbands\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \"\"\"\n\u001b[0;32m-> 1258\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mImageMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/ImageMode.py\u001b[0m in \u001b[0;36mgetmode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m\"\"\"Gets a mode descriptor for the given mode.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_modes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m# initialize mode cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mmodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='../dataset/lab03', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='../dataset/lab03', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Average pool 1\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Average pool 2\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Average pool 1\n",
    "        out = self.avgpool1(out)\n",
    "\n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Max pool 2 \n",
    "        out = self.avgpool2(out)\n",
    "\n",
    "        # Resize\n",
    "        # Original size: (100, 32, 7, 7)\n",
    "        # out.size(0): 100\n",
    "        # New out size: (100, 32*7*7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5d_bP9b1A5Fs"
   },
   "source": [
    "> **Comparison of accuracies**\n",
    ">\n",
    "> It seems like average pooling test accuracy is less than the max pooling accuracy! Does this mean average pooling is better? This is not definitive and depends on a lot of factors including the model's architecture, seed (that affects random weight initialization) and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "09ooCflcA5Ft"
   },
   "source": [
    "### Average Pooling Test Accuracy < Max Pooling Test Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rAFsRJU8A5Ft"
   },
   "source": [
    "### Model C: \n",
    "- 2 Convolutional Layers\n",
    "    - **Valid Padding** (smaller output size)\n",
    "- 2 **Max Pooling** Layers\n",
    "- 1 Fully Connected Layer\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1tPKsY1gfbjsjk3hVA_K2U5ZRNCEIVrKl\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1K4L289pUYpLPis0xAy54lHRTcAa03N8G\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- Step 3: Create Model Class\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x65XdvYGA5Fu"
   },
   "source": [
    "**2 Conv + 2 Max Pool + 1 FC (Valid Padding, No Padding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1OG2drgdA5Fv",
    "outputId": "8c7e34a5-825d-4b4d-d2f1-ca84dae55819"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='../dataset/lab03', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='../dataset/lab03', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "\n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "\n",
    "        # Resize\n",
    "        # Original size: (100, 32, 7, 7)\n",
    "        # out.size(0): 100\n",
    "        # New out size: (100, 32*7*7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "               \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndfLOe9lA5F0"
   },
   "source": [
    "### Summary of Results\n",
    "\n",
    "| Model A | Model B | Model C | \n",
    "|------|------|------|\n",
    "|   Max Pooling  | Average Pooling | Max Pooling |\n",
    "| Same Padding | Same Padding | Valid Padding | \n",
    "| 97.04% | 93.59% | 96.5% | \n",
    "\n",
    "\n",
    "| All Models |\n",
    "|------|\n",
    "| INPUT $\\rightarrow$ CONV $\\rightarrow$ POOL $\\rightarrow$ CONV $\\rightarrow$ POOL $\\rightarrow$ FC | \n",
    "| Convolution Kernel Size = 5 x 5 |\n",
    "| Convolution Kernel Stride = 1 |\n",
    "| Pooling Kernel Size = 2 x 2 |\n",
    "\n",
    "### General Deep Learning Notes on CNN and FNN\n",
    "- 3 ways to expand a convolutional neural network\n",
    "    - More convolutional layers \n",
    "    - Less aggressive downsampling\n",
    "        - Smaller kernel size for pooling (gradually downsampling)\n",
    "    - More fully connected layers \n",
    "- Cons\n",
    "    - Need a larger dataset\n",
    "        - Curse of dimensionality\n",
    "    - Does not necessarily mean higher accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIUg4BtXA5F0"
   },
   "source": [
    "## 3. Building a Convolutional Neural Network with PyTorch (GPU)\n",
    "### Model A\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1xQzHZ7okBvBGmRgfLH1dd95onwdTG6Gv\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1EVR_GeSZhE_gRPw0kjDtzd0rVG9ccuOU\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "GPU: 2 things must be on GPU\n",
    "- `model`\n",
    "- `tensors`\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- Step 3: Create Model Class\n",
    "- **Step 4: Instantiate Model Class**\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- **Step 7: Train Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Is9HbJQJA5F0"
   },
   "source": [
    "**2 Conv + 2 Max Pooling + 1 FC (Same Padding, Zero Padding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kw2ghnLdA5F1",
    "outputId": "95bf90a2-e488-4600-ee64-811f380b06d8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets \n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='../dataset/lab03', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='../dataset/lab03', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "\n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "\n",
    "        # Resize\n",
    "        # Original size: (100, 32, 7, 7)\n",
    "        # out.size(0): 100\n",
    "        # New out size: (100, 32*7*7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                # Total correct predictions\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASpRhzDXA5F5"
   },
   "source": [
    "> **More Efficient Convolutions via Toeplitz Matrices**\n",
    ">\n",
    "> This is beyond the scope of this particular lesson. But now that we understand how convolutions work, it is critical to know that it is quite an inefficient operation if we use for-loops to perform our 2D convolutions (5 x 5 convolution kernel size for example) on our 2D images (28 x 28 MNIST image for example).\n",
    ">\n",
    "> A more efficient implementation is in converting our convolution kernel into a Toeplitz matrix and our image into a vector. Then, we will do just one matrix operation using our Toeplitz matrix and vector.\n",
    ">\n",
    "> There will be a whole lesson dedicated to this operation released down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kNSaJTzOA5F5"
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Transition from **Feedforward Neural Network**\n",
    "    - Addition of **Convolutional** & **Pooling** Layers before Linear Layers\n",
    "- One **Convolutional** Layer Basics\n",
    "- One **Pooling** Layer Basics\n",
    "    - Max pooling\n",
    "    - Average pooling\n",
    "- **Padding**\n",
    "- **Output Dimension** Calculations and Examples\n",
    "    -  $ O = \\frac {W - K + 2P}{S} + 1$\n",
    "- Convolutional Neural Networks\n",
    "    - **Model A**: 2 Conv + 2 Max pool + 1 FC\n",
    "        - Same Padding\n",
    "    - **Model B**: 2 Conv + 2 Average pool + 1 FC\n",
    "        - Same Padding\n",
    "    - **Model C**: 2 Conv + 2 Max pool + 1 FC\n",
    "        - Valid Padding\n",
    "- Model Variation in **Code**\n",
    "    - Modifying only step 3\n",
    "- Ways to Expand Models **Capacity**\n",
    "    - More convolutions\n",
    "    - Gradual pooling\n",
    "    - More fully connected layers\n",
    "- **GPU** Code\n",
    "    - 2 things on GPU\n",
    "        - **model**\n",
    "        - **variable**\n",
    "    - Modifying only **Step 4 & Step 7**\n",
    "- **7 Step** Model Building Recap\n",
    "    - Step 1: Load Dataset\n",
    "    - Step 2: Make Dataset Iterable\n",
    "    - Step 3: Create Model Class\n",
    "    - Step 4: Instantiate Model Class\n",
    "    - Step 5: Instantiate Loss Class\n",
    "    - Step 6: Instantiate Optimizer Class\n",
    "    - Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wT_7e4eTA5F6"
   },
   "source": [
    "### *References*\n",
    "[1] [DOI](https://zenodo.org/badge/139945544.svg)(https://zenodo.org/badge/latestdoi/139945544)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "EEE4423_lab3_cnns.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Full on Python 3.6 (GPU)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
